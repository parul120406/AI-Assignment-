# AI-Assignment-
Topic : "Retail Customer Churn Analysis"
Group Members :
Parul Gupta KU2407U491
Shailja Jagani KU2407U492
Shubhpuri Goswami KU2407U493
Vansh Kanodiya KU2407U494

Objective of the Project:
The goal of this project is to analyze customer churn in a retail setting using machine learning techniques. We aim to identify factors that contribute to customer attrition and predict which customers are at high risk of churning, enabling businesses to take proactive measures to retain valuable customers.

 Tools and Libraries Used:
-*Programming Language*: Python
- Google Colab
- Pandas: For data manipulation and analysis
- Matplotlib: For data visualization

Data Source(s):
- The dataset used for this project is sourced from a publicly available retail customer churn dataset. It contains customer demographics, transaction history, and 
  churn status.
- Data Link: [Kaggle: Retail Customer Churn] (https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

Execution Steps: 
Step 1: Open Google Colab
Go to Google Colab.
Create a new notebook by clicking on "New Notebook".

Step 2: Upload the Data File
Click on the folder icon on the left sidebar in Colab to open the file browser.
Click on the Upload icon (shaped like a file with an arrow pointing up).
Upload your data.csv file.

Step 3: Install Dependencies
Run the Code to install - matplotlib.pyplot , pandas

Step 4: Write the Code 
Write the required Python Code

Step 5: Execute the Code
Press Shift + Enter or click the Run button on the code cell to execute it.

Step 6: View the Output
The bar plot visualizing Customer Churn Distribution will be displayed as the output

Summary of the Result :
The graph shows that most customers (around 5000) were retained, while a smaller portion (around 1500) churned. This indicates that the majority of customers are loyal, but the churn rate of approximately 23% suggests there is still room to improve retention strategies to reduce customer loss.

Challenges Faced :  
- Balancing between insightful visualizations and avoiding information overload.  
- Time constraints in building predictive models

  

